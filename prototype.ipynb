{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Imports </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_datareader.data as web\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import ensemble\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "import yfinance as yf\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> 0. Data import </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(start_date, end_date, ticker, df):\n",
    "    idx = pd.IndexSlice\n",
    "    new_df = df.loc[idx[start_date:end_date, ticker], :]\n",
    "    return new_df\n",
    "\n",
    "df = (pd.read_csv('data/wiki_stocks.csv',\n",
    "                 parse_dates=['date'],\n",
    "                 index_col=['date', 'ticker'],\n",
    "                 infer_datetime_format=True)\n",
    "     .sort_index())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 1. Time series naive </h4>\n",
    "Wrapper function for all models using naive time series data (predicting price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> 1.1 ARIMA </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "import pandas_datareader.data as web\n",
    "import numpy as np\n",
    "from numpy.linalg import LinAlgError\n",
    "\n",
    "import statsmodels.tsa.api as tsa\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import acf, q_stat, adfuller\n",
    "from scipy.stats import probplot, moment\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# Make a function to find the MSE of a single ARIMA model \n",
    "def evaluate_arima_model(data, arima_order):\n",
    "    # Needs to be an integer because it is later used as an index.\n",
    "    split=int(len(data) * 0.8) \n",
    "    train, test = data[0:split], data[split:len(data)]\n",
    "    past=[x for x in train]\n",
    "    # make predictions\n",
    "    predictions = list()\n",
    "    for i in range(len(test)):#timestep-wise comparison between test data and one-step prediction ARIMA model. \n",
    "        model = ARIMA(past, order=arima_order)\n",
    "        model_fit = model.fit()\n",
    "        future = model_fit.forecast()[0]\n",
    "        predictions.append(future)\n",
    "        past.append(test[i])\n",
    "    # calculate out of sample error\n",
    "    error = mean_squared_error(test, predictions)\n",
    "    return error\n",
    "# Make a function to evaluate different ARIMA models with several different p, d, and q values.\n",
    "\n",
    "def evaluate_models(dataset, p_values, d_values, q_values):\n",
    "    best_score, best_cfg = float(\"inf\"), None\n",
    "    for p in p_values:\n",
    "        for d in d_values:\n",
    "            for q in q_values:\n",
    "                order = (p,d,q)\n",
    "                try:\n",
    "                    mse = evaluate_arima_model(dataset, order)\n",
    "                    if mse < best_score:\n",
    "                        best_score, best_cfg = mse, order\n",
    "                    print('ARIMA%s MSE=%.10f' % (order,mse))\n",
    "                except:\n",
    "                    continue\n",
    "    return print('Best ARIMA%s MSE=%.10f' % (best_cfg, best_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we choose a couple of values to try for each parameter.\n",
    "# p_values = [x for x in range(0, 3)]\n",
    "# d_values = [x for x in range(0, 3)]\n",
    "# q_values = [x for x in range(0, 3)]\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "# evaluate_models(arima_log, p_values, d_values, q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_arima(data):\n",
    "    # Calculate ARIMA w/ TS data and print results / return results\n",
    "    log_dat = np.log(data.reset_index(level = [1])['open'])\n",
    "\n",
    "    model = ARIMA(log_dat, order=(0, 2, 1))\n",
    "    model_fit = model.fit()\n",
    "    forecast = model_fit.forecast(24)\n",
    "\n",
    "    print(model_fit.summary())\n",
    "\n",
    "    preds = model_fit.predict()\n",
    "\n",
    "    plt.figure(figsize=(15,10))\n",
    "    plt.plot(preds[1:], color = 'red')\n",
    "    plt.plot(log_dat[1:])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> 1.2 RNN </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_univariate_rnn_data(data, window_size):\n",
    "    n = len(data)\n",
    "    y = data[window_size:]\n",
    "    data = data.values.reshape(-1, 1) # make 2D\n",
    "    X = np.hstack(tuple([data[i: n-j, :] for i, j in enumerate(range(window_size, 0, -1))]))\n",
    "    return pd.DataFrame(X, index=y.index), y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_rnn(data):\n",
    "    # Calculate rnn w/ TS data and print results / return results\n",
    "\n",
    "    results_path = Path('results', 'univariate_time_series')\n",
    "    if not results_path.exists():\n",
    "        results_path.mkdir(parents=True)\n",
    "\n",
    "    dat = data.reset_index(level = [1])[['open']]\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_data = pd.Series(scaler.fit_transform(dat).squeeze(), index=dat.index)\n",
    "\n",
    "    window_size = 63\n",
    "\n",
    "    ### Train-test split ###\n",
    "\n",
    "    X, y = create_univariate_rnn_data(scaled_data, window_size=window_size)\n",
    "\n",
    "    split=int(len(scaled_data) * 0.8) \n",
    "\n",
    "    ### Train on data up to 2018 ###\n",
    "    X_train = X[:split].values.reshape(-1, window_size, 1)\n",
    "    y_train = y[:split]\n",
    "\n",
    "    ### Test on data from 2019 ###\n",
    "    X_test = X[split:len(scaled_data)].values.reshape(-1, window_size, 1)\n",
    "    y_test = y[split:len(scaled_data)]\n",
    "\n",
    "    n_obs, window_size, n_features = X_train.shape\n",
    "\n",
    "    rnn = Sequential([\n",
    "        LSTM(units=10, \n",
    "            input_shape=(window_size, n_features), name='LSTM'),\n",
    "        Dense(1, name='Output')\n",
    "    ])\n",
    "\n",
    "    ### Define optimizer for RNN ###\n",
    "\n",
    "    optimizer = tf.keras.optimizers.legacy.RMSprop(learning_rate=0.001,\n",
    "                                        rho=0.9,\n",
    "                                        epsilon=1e-08,\n",
    "                                        decay=0.0)\n",
    "\n",
    "    ### Compile RNN w/ previously defined optimizer ###\n",
    "    rnn.compile(loss='mean_squared_error', \n",
    "                optimizer=optimizer)\n",
    "\n",
    "    ### Save best path of model during training ###\n",
    "    rnn_path = (results_path / 'rnn.h5').as_posix()\n",
    "\n",
    "    ### Call best model during training ###\n",
    "    checkpointer = ModelCheckpoint(filepath=rnn_path,\n",
    "                                verbose=0,\n",
    "                                monitor='val_loss',\n",
    "                                save_best_only=True)\n",
    "\n",
    "    ### Early stopping to prevent overfitting ###\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', \n",
    "                                patience=20,\n",
    "                                restore_best_weights=True)\n",
    "\n",
    "    ### Train model ###\n",
    "    lstm_training = rnn.fit(X_train,\n",
    "                            y_train,\n",
    "                            epochs=150,\n",
    "                            batch_size=20,\n",
    "                            shuffle=True,\n",
    "                            validation_data=(X_test, y_test),\n",
    "                            callbacks=[early_stopping, checkpointer],\n",
    "                            verbose=0)\n",
    "\n",
    "    ### More performance benchmarks ###\n",
    "\n",
    "    train_rmse_scaled = np.sqrt(rnn.evaluate(X_train, y_train, verbose=0))\n",
    "    test_rmse_scaled = np.sqrt(rnn.evaluate(X_test, y_test, verbose=0))\n",
    "    print(f'Train RMSE: {train_rmse_scaled:.4f} | Test RMSE: {test_rmse_scaled:.4f}')\n",
    "\n",
    "    ### We trained on scaled data, therefore our predictions are in scaled form ###\n",
    "    train_predict_scaled = rnn.predict(X_train)\n",
    "    test_predict_scaled = rnn.predict(X_test)\n",
    "\n",
    "    ### Correlation coefficient between predicted and actual values ###\n",
    "    train_ic = spearmanr(y_train, train_predict_scaled)[0]\n",
    "    test_ic = spearmanr(y_test, test_predict_scaled)[0]\n",
    "    print(f'Train IC: {train_ic:.4f} | Test IC: {test_ic:.4f}')\n",
    "\n",
    "    plt.figure(figsize=(15,10))\n",
    "    plt.plot(test_predict_scaled[1:], color = 'red')\n",
    "    plt.plot(y_test.values[1:])\n",
    "\n",
    "    return rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_data('2008-01-01', '2018-01-01', \"AAPL\", df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> 1.3 Gradient Boosting </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_gb(data):\n",
    "    # Calculate XGBoost w/ TS data and print results / return results\n",
    "\n",
    "    gb_dat = data.reset_index(level = [1])['open']\n",
    "\n",
    "    gb_df = gb_dat.reset_index()\n",
    "    gb_df.columns = ['date', 'open']\n",
    "\n",
    "    gb_df['timestamp'] = (gb_df['date'].astype('int64') // 10**9).astype('int32')\n",
    "    gb_df = gb_df[['open', 'timestamp']]\n",
    "\n",
    "    dataset = gb_df.values\n",
    "    X = dataset[:,1].reshape(-1,1)\n",
    "    Y = dataset[:,0:1]\n",
    "\n",
    "    validation_size = 0.15\n",
    "    seed = 7\n",
    "\n",
    "    X_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=validation_size, random_state=seed)\n",
    "\n",
    "    params = {'n_estimators': 500, 'max_depth': 4, 'min_samples_split': 2,\n",
    "          'learning_rate': 0.01}\n",
    "    model = ensemble.GradientBoostingRegressor(**params)\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    model_score = model.score(X_train, Y_train)\n",
    "    print('R2 sq: ',model_score)\n",
    "    y_predicted = model.predict(X_validation)\n",
    "    # The mean squared error\n",
    "    print(\"Mean squared error: %.2f\"% mean_squared_error(Y_validation, y_predicted))\n",
    "    # Explained variance score: 1 is perfect prediction\n",
    "    print('Test Variance score: %.2f' % r2_score(Y_validation, y_predicted))\n",
    "    \n",
    "    plt.figure(figsize=(15,10))\n",
    "    plt.plot(y_predicted[1:], color = 'red')\n",
    "    plt.plot(Y_validation[1:])\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> 1.4 Q Learning </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_rl(data):\n",
    "    # Calculate Q learning w/ TS data and print results / return results\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> 1.5 KNN </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_knn(data):\n",
    "    # Calculate knn w/ TS data and print results / return results\n",
    "    data['Open-Close'] = data.Open - data.Close\n",
    "    data['High-Low'] = data.High - data.Low\n",
    "    data = data.dropna()\n",
    "\n",
    "    X = data[['Open-Close', 'High-Low']]\n",
    "\n",
    "    Y = data['Open'].shift(-1)\n",
    "\n",
    "    split_percentage = 0.8\n",
    "    split = int(split_percentage*len(data))\n",
    "\n",
    "    X_train = X[:split]\n",
    "    Y_train = Y[:split]\n",
    "\n",
    "    X_test = X[split:-1]\n",
    "    Y_test = Y[split:-1]\n",
    "\n",
    "    # Instantiate KNN learning model(k=15)\n",
    "    knn = KNeighborsRegressor(n_neighbors=15)\n",
    "\n",
    "    # fit the model\n",
    "    knn.fit(X_train, Y_train)\n",
    "\n",
    "    y_preds = knn.predict(X_test)\n",
    "    print(\"MSE: \", mean_squared_error(y_preds, Y_test))\n",
    "\n",
    "    return knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> 1.6 Naive Bayes </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_nb(data):\n",
    "    # Calculate naive bayes w/ TS data and print results / return results\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "aapl = yf.download(\"AAPL\", start=\"2010-01-01\", end=\"2020-01-01\")\n",
    "\n",
    "aapl = aapl.dropna()\n",
    "\n",
    "X = aapl[['Open']]\n",
    "\n",
    "Y = aapl['Open'].shift(-1)\n",
    "\n",
    "split_percentage = 0.8\n",
    "split = int(split_percentage*len(aapl))\n",
    "\n",
    "X_train = X[:split]\n",
    "Y_train = Y[:split]\n",
    "\n",
    "X_test = X[split:-1]\n",
    "Y_test = Y[split:-1]\n",
    "\n",
    "# Instantiate KNN learning model(k=15)\n",
    "CLF = GaussianNB()\n",
    "\n",
    "# fit the model\n",
    "CLF.fit(X_train, Y_train)\n",
    "\n",
    "y_preds = CLF.predict(X_test)\n",
    "print(\"MSE: \", mean_squared_error(y_preds, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> 1.7 VAE </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_vae(data):\n",
    "    # Calculate VAE w/ TS data and print results / return results\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> 1.8 Prophet </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_prophet(data):\n",
    "    # Calculate Prophet for seasonality w/ TS data and print results / return results\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> 1.9 Ensemble </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_ensemble(data):\n",
    "    # Use ensemble of multiple models, some combination of above (NOT RL)\n",
    "    #  w/ TS data and print results / return results\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_price(model_selection, ticker, date_start, date_end):\n",
    "    data = get_data(date_start, date_end, ticker, df)\n",
    "    if model_selection == \"ARIMA\":\n",
    "        ts_arima(data)\n",
    "        pass\n",
    "    elif model_selection == \"RNN\":\n",
    "        ts_rnn(data)\n",
    "        pass\n",
    "    elif model_selection == \"GB\":\n",
    "        ts_gb(data)\n",
    "        pass\n",
    "    elif model_selection == \"RL\":\n",
    "        ts_rl(data)\n",
    "        pass\n",
    "    elif model_selection == \"KNN\":\n",
    "        knn_dat = yf.download(ticker, start=date_start, end=date_end)\n",
    "        ts_knn(knn_dat)\n",
    "        pass\n",
    "    elif model_selection == \"NB\":\n",
    "        ts_nb(data)\n",
    "        pass\n",
    "    elif model_selection == \"VAE\":\n",
    "        ts_vae(data)\n",
    "        pass\n",
    "    elif model_selection == \"PROPHET\":\n",
    "        ts_prophet(data)\n",
    "        pass\n",
    "    elif model_selection == \"ENSEMBLE\":\n",
    "        ts_ensemble(data)\n",
    "        pass\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "192.2655650876487\n"
     ]
    }
   ],
   "source": [
    "ts_price(\"KNN\", \"AAPL\", '2008-01-01', '2018-01-01')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> 2. Combining other techniques </h4>\n",
    "\n",
    "<ol>\n",
    "    <li> Incorporating NLP </li>\n",
    "    Use twitter data for stocks from previous research\n",
    "    <li> Use returns for evaluation </li>\n",
    "    See david's notes\n",
    "    <li> Predicting indicators such as RSI, Momentum </li>\n",
    "    See stefan jansen's work\n",
    "    <li> Non-traditional TS models </li>\n",
    "    For example, clustering methods? Neural transformers? Classification models? Weighted precision and measure for metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO-DOS: \n",
    "\n",
    "1. Transition to using yahoo finance \n",
    "aapl = yf.download(\"AAPL\", start=\"2020-01-01\", end=\"2021-01-01\")\n",
    "\n",
    "stocks = [\"AAPL\", \"AMZN\", \"MSFT\"]\n",
    "data = yf.download(stocks, start=\"2020-01-01\", end=\"2021-01-01\")\n",
    "\n",
    "2. Compile hyperparameter tuning, right now from a bunch of scattered sources, so hard coded in. Add maybe an additional section to grid search or something.\n",
    "\n",
    "3. Make sure MSE uniform. To do so, ensure data set size is same for all models. Maybe have average MSE by test set size?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
